# @package _global_

defaults:
  - override /preprocess: porphyry-copper-natl.yaml
  - override /model: maevit_classifier.yaml
  - override /trainer: gpu #ddp
  - override /callbacks: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["cu", "ViT", "cont-US", "l22", "pretrained", "mitre-data"]
task_name: "cmta3-classifier-cu-mitre"

seed: 1234
enable_attributions: true

data:
  window_size: 5
  tif_dir: ${paths.data_dir}/H3/Porphyry-Copper
  likely_neg_range: [0.3, 0.5]
  # coordinates for MITRE 80% training set
  specified_split: [[279.0, 26.0], [246.0, 38.0], [212.0, 50.0], [196.0, 75.0], [201.0, 86.0], [163.0, 138.0], [476.0, 173.0], [468.0, 179.0], [476.0, 203.0], [478.0, 210.0], [491.0, 214.0], [462.0, 225.0], [408.0, 250.0], [541.0, 260.0], [515.0, 274.0], [529.0, 279.0], [538.0, 285.0], [335.0, 307.0], [540.0, 312.0], [530.0, 323.0], [538.0, 331.0], [538.0, 343.0], [534.0, 344.0], [548.0, 351.0], [549.0, 353.0], [475.0, 384.0], [345.0, 431.0], [117.0, 463.0], [254.0, 470.0], [247.0, 473.0], [254.0, 474.0], [251.0, 477.0], [246.0, 483.0], [226.0, 486.0], [320.0, 501.0], [338.0, 507.0], [424.0, 512.0], [452.0, 512.0], [311.0, 530.0], [176.0, 534.0], [158.0, 536.0], [160.0, 537.0], [154.0, 538.0], [419.0, 547.0], [631.0, 552.0], [291.0, 556.0], [311.0, 559.0], [201.0, 561.0], [182.0, 565.0], [211.0, 587.0], [617.0, 605.0], [617.0, 643.0], [566.0, 665.0], [565.0, 672.0], [546.0, 696.0], [602.0, 702.0], [301.0, 765.0], [303.0, 766.0], [316.0, 782.0], [608.0, 805.0], [331.0, 812.0], [607.0, 813.0], [354.0, 821.0], [381.0, 827.0], [350.0, 831.0], [359.0, 846.0], [277.0, 852.0], [411.0, 888.0], [407.0, 892.0], [409.0, 892.0], [381.0, 899.0], [401.0, 900.0], [404.0, 900.0], [386.0, 904.0], [370.0, 905.0], [394.0, 905.0], [413.0, 907.0], [367.0, 908.0], [612.0, 911.0], [468.0, 913.0], [456.0, 919.0], [357.0, 923.0], [459.0, 923.0], [421.0, 924.0], [413.0, 926.0], [363.0, 927.0], [323.0, 929.0], [539.0, 929.0], [516.0, 933.0], [377.0, 935.0], [516.0, 936.0], [409.0, 937.0], [512.0, 939.0], [414.0, 942.0], [503.0, 942.0], [486.0, 957.0], [391.0, 958.0], [432.0, 960.0], [433.0, 961.0], [573.0, 961.0], [474.0, 962.0], [391.0, 963.0], [592.0, 963.0], [432.0, 964.0], [388.0, 965.0], [392.0, 965.0], [402.0, 970.0], [414.0, 975.0], [439.0, 978.0], [401.0, 987.0], [400.0, 989.0], [400.0, 992.0], [434.0, 995.0], [648.0, 1105.0]]
  frac_train_split: 0.9

model:
  optimizer:
    _target_: torch.optim.AdamW
    lr: 1e-3
    weight_decay: 1e-2
  net:
    _target_: sri_maper.src.models.mae_vit_classifier.CLSClassifier
    backbone_ckpt: null
    backbone_net:
      _target_: sri_maper.src.models.mae_vit.MAE_ViT
      image_size: ${data.window_size}
      patch_size: 1
      input_dim: 23
      enc_dim: 256
      encoder_layer: 6
      encoder_head: 8
      dec_dim: 128
      output_dim: ${model.net.backbone_net.input_dim}
      decoder_layer: 2
      decoder_head: 4
      mask_ratio: 0.0
    freeze_backbone: true
    dropout_rate: [0.0, 0.0, 0.4]
  compile: false
  mc_samples: 100
  gain: 1.0
  smoothing: 0.3
  extract_attributions: ${enable_attributions}

trainer:
  min_epochs: 50
  max_epochs: 100
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  limit_test_batches: 1.0
  val_check_interval: 0.5

logger:
  wandb:
    name: "ViT|pretrained|frozen"

callbacks:
  model_checkpoint:
    monitor: val/auprc
  early_stopping:
    monitor: val/auprc_best
