# @package _global_

defaults:
  - override /model: maevit_pretrain
  - override /trainer: ddp
  - override /callbacks: default

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["vit", "mae-vit-pretrain", "cont-US", "l22", "maniac"]
task_name: "cmta3-pretrain-maevit"

seed: 1234

data:
  window_size: 5
  batch_size: 1024
  tif_dir: ${paths.data_dir}/H3_dilation_5/MaNiAC
  downsample: false
  oversample: false

model:
  optimizer:
    _target_: torch.optim.AdamW
    lr: 1e-3
    weight_decay: 1e-2
  warmup_epoch: 1
  net:
    _target_: sri_maper.src.models.mae_vit.MAE_ViT
    image_size: ${data.window_size}
    patch_size: 1
    input_dim: 14
    enc_dim: 256
    encoder_layer: 6
    encoder_head: 8
    dec_dim: 128
    output_dim: ${model.net.input_dim}
    decoder_layer: 2
    decoder_head: 4
    mask_ratio: 0.75
  compile: false
  mc_samples: 100

trainer:
  min_epochs: 10
  max_epochs: 30
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  limit_test_batches: 1.0
  val_check_interval: 0.5
  gradient_clip_val: 1.0

logger:
  wandb:
    name: "maevit_pretrain_l22_uscont_maniac"

callbacks:
  model_checkpoint:
    filename: "ssim_{val/ssim:.3f}"
    monitor: val/ssim
  early_stopping:
    monitor: val/ssim_best
