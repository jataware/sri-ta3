# @package _global_

defaults:
  - override /preprocess: tungsten-skarn-natl.yaml
  - override /model: resnet.yaml
  - override /trainer: ddp
  - override /callbacks: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["W", "resnet", "cont-US", "l22"]
task_name: "cmta3-classifier-W"

seed: 1234
extract_attributions: false

data:
  window_size: 5
  tif_dir: ${paths.data_dir}/H3_dilation_5/Tungsten-Skarn
  likely_neg_range: [0.25,0.75]
  frac_train_split: 0.5

model:
  optimizer:
    _target_: torch.optim.AdamW
    lr: 1e-3
    weight_decay: 1e-2
  net:
    _target_: sri_maper.src.models.resnet.ResNet
    num_input_channels: 77
    num_output_classes: 1
    dropout_rate: 0.5
  compile: false
  gain: 1.0
  mc_samples: 100
  smoothing: 0.2
  extract_attributions: ${extract_attributions}

trainer:
  min_epochs: 15
  max_epochs: 100
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  limit_test_batches: 1.0
  val_check_interval: 0.5

logger:
  wandb:
    name: "resnet|l22|uscont"

callbacks:
  model_checkpoint:
    monitor: val/auprc
  early_stopping:
    monitor: val/auprc_best
